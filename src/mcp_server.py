from mcp.server.fastmcp import FastMCP
from dotenv import load_dotenv
import os
import sys
import json
from datetime import datetime

# Ensure src path for imports
sys.path.append(os.path.abspath("src"))
# Ensure project root for Memory_System
sys.path.append(os.path.dirname(os.path.abspath(__file__))) # src/
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))) # Repo Root
from tools.workbench_bridge import WorkbenchBridge
from tools.workbench_bridge import WorkbenchBridge
from composio import Composio

# Memory System Integration
try:
    from Memory_System.manager import MemoryManager
    MEMORY_MANAGER = MemoryManager(storage_dir=os.path.join(os.path.dirname(__file__), "..", "Memory_System_Data"))
    sys.stderr.write("[Local Toolkit] Memory System initialized.\n")
except Exception as e:
    sys.stderr.write(f"[Local Toolkit] Memory System init failed: {e}\n")
    MEMORY_MANAGER = None

# Initialize Configuration
load_dotenv()

# Configure Logfire for MCP observability
try:
    import logfire
    if os.getenv("LOGFIRE_TOKEN"):
        logfire.configure(
            service_name="local-toolkit",
            send_to_logfire="if-token-present",
        )
        logfire.instrument_mcp()
        sys.stderr.write("[Local Toolkit] Logfire instrumentation enabled\\n")
except ImportError:
    pass

try:
    sys.stderr.write("[Local Toolkit] Server starting components...\n")
    mcp = FastMCP("Local Intelligence Toolkit")
except Exception:
    raise


def get_bridge():
    client = Composio(api_key=os.environ.get("COMPOSIO_API_KEY"))
    return WorkbenchBridge(composio_client=client, user_id="user_123")


def fix_path_typos(path: str) -> str:
    """
    Fix common model typos in workspace paths.
    Models sometimes truncate 'AGENT_RUN_WORKSPACES' to 'AGENT_RUNSPACES'.
    """
    # Fix: AGENT_RUNSPACES -> AGENT_RUN_WORKSPACES
    if "AGENT_RUNSPACES" in path and "AGENT_RUN_WORKSPACES" not in path:
        path = path.replace("AGENT_RUNSPACES", "AGENT_RUN_WORKSPACES")
        sys.stderr.write(f"[Local Toolkit] Path auto-corrected: AGENT_RUNSPACES â†’ AGENT_RUN_WORKSPACES\n")
    return path


@mcp.tool()
def workbench_download(
    remote_path: str, local_path: str, session_id: str = None
) -> str:
    """
    Download a file from the Remote Composio Workbench to the Local Workspace.
    """
    bridge = get_bridge()
    result = bridge.download(remote_path, local_path, session_id=session_id)
    if result.get("error"):
        return f"Error: {result['error']}"
    return f"Successfully downloaded {remote_path} to {local_path}. Local path: {result.get('local_path')}"


@mcp.tool()
def workbench_upload(local_path: str, remote_path: str, session_id: str = None) -> str:
    """
    Upload a file from the Local Workspace to the Remote Composio Workbench.
    """
    bridge = get_bridge()
    result = bridge.upload(local_path, remote_path, session_id=session_id)
    if result.get("error"):
        return f"Error: {result['error']}"
    return f"Successfully uploaded {local_path} to {remote_path}."


@mcp.tool()
def read_local_file(path: str) -> str:
    """
    Read content from a file in the Local Workspace.
    """
    try:
        path = fix_path_typos(path)  # Auto-correct common model typos
        abs_path = os.path.abspath(path)
        if not os.path.exists(abs_path):
            return f"Error: File not found at {path}"

        with open(abs_path, "r", encoding="utf-8") as f:
            content = f.read()
        return content
    except Exception as e:
        return f"Error reading file: {str(e)}"


@mcp.tool()
def write_local_file(path: str, content: str) -> str:
    """
    Write content to a file in the Local Workspace.
    Useful for saving reports, summaries, or code generated by sub-agents.
    """
    try:
        abs_path = os.path.abspath(path)
        os.makedirs(os.path.dirname(abs_path), exist_ok=True)
        with open(abs_path, "w", encoding="utf-8") as f:
            f.write(content)
        return f"Successfully wrote {len(content)} chars to {path}"
    except Exception as e:
        return f"Error writing file: {str(e)}"


@mcp.tool()
def list_directory(path: str) -> str:
    """
    List contents of a directory in the Local Workspace.
    """
    try:
        path = fix_path_typos(path)  # Auto-correct common model typos
        abs_path = os.path.abspath(path)
        if not os.path.exists(abs_path):
            return f"Error: Directory not found at {path}"
        
        items = os.listdir(abs_path)
        return json.dumps(items, indent=2)
    except Exception as e:
        return f"Error listing directory: {str(e)}"



@mcp.tool()
def compress_files(files: list[str], output_archive: str) -> str:
    """
    Compress a list of files into a zip archive.
    Args:
        files: List of absolute file paths to include.
        output_archive: Absolute path for the output zip file.
    """
    import zipfile
    try:
        # Validate input paths
        validated_files = []
        for f in files:
            abs_path = os.path.abspath(f)
            if not os.path.exists(abs_path):
                return json.dumps({"error": f"File not found: {f}"})
            validated_files.append(abs_path)

        output_path = os.path.abspath(output_archive)
        
        # Create parent directory if needed
        os.makedirs(os.path.dirname(output_path), exist_ok=True)

        with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zf:
            for file_path in validated_files:
                # Arcname is the name inside the zip file (basename)
                zf.write(file_path, arcname=os.path.basename(file_path))
                
        # Check if created
        if os.path.exists(output_path):
             size = os.path.getsize(output_path)
             return json.dumps({
                 "success": True, 
                 "archive_path": output_path,
                 "size_bytes": size,
                 "files_included": len(validated_files)
             })
        return json.dumps({"error": "Failed to create archive file"})

    except Exception as e:
        return json.dumps({"error": f"Compression failed: {str(e)}"})


@mcp.tool()
def upload_to_composio(
    path: str, 
    tool_slug: str = "GMAIL_SEND_EMAIL",
    toolkit_slug: str = "gmail"
) -> str:
    """
    Upload a local file to Composio S3 for use as an email attachment or other tool input.
    Uses native Composio SDK FileUploadable.from_path() - the correct, supported method.
    
    Args:
        path: Absolute path to the local file to upload
        tool_slug: The Composio tool that will consume this file (default: GMAIL_SEND_EMAIL)
        toolkit_slug: The toolkit the tool belongs to (default: gmail)
    
    Returns JSON with:
    - s3key: ID for tool attachments (pass to Gmail/Slack)
    - mimetype: Detected file type
    - name: Original filename
    """
    try:
        abs_path = os.path.abspath(path)
        if not os.path.exists(abs_path):
            return json.dumps({"error": f"File not found: {path}"})
        
        # Import native Composio file helper
        from composio.core.models._files import FileUploadable
        
        # Get Composio client
        client = Composio(api_key=os.environ.get("COMPOSIO_API_KEY"))
        
        # Use native SDK method - this is the correct approach per Composio docs
        sys.stderr.write(f"[upload_to_composio] Uploading {abs_path} via native FileUploadable.from_path()\\n")
        
        result = FileUploadable.from_path(
            client=client.client,
            file=abs_path,
            tool=tool_slug,
            toolkit=toolkit_slug
        )
        
        # Return the attachment-ready format
        response = {
            "s3key": result.s3key,
            "mimetype": result.mimetype,
            "name": result.name,
            "local_path": abs_path
        }
        
        sys.stderr.write(f"[upload_to_composio] SUCCESS: s3key={result.s3key}\\n")
        return json.dumps(response, indent=2)
        
    except Exception as e:
        import traceback
        sys.stderr.write(f"[upload_to_composio] ERROR: {traceback.format_exc()}\\n")
        return json.dumps({"error": str(e)})

# =============================================================================
# MEMORY SYSTEM TOOLS
# =============================================================================

@mcp.tool()
def core_memory_replace(label: str, new_value: str) -> str:
    """
    Overwrite a Core Memory block (e.g. 'human', 'persona').
    Use this to update persistent facts about the user or yourself.
    """
    if not MEMORY_MANAGER:
        return "Error: Memory System not initialized."
    return MEMORY_MANAGER.core_memory_replace(label, new_value)

@mcp.tool()
def core_memory_append(label: str, text_to_append: str) -> str:
    """
    Append text to a Core Memory block.
    Useful for adding a new preference without deleting old ones.
    """
    if not MEMORY_MANAGER:
         return "Error: Memory System not initialized."
    return MEMORY_MANAGER.core_memory_append(label, text_to_append)

@mcp.tool()
def archival_memory_insert(content: str, tags: str = "") -> str:
    """
    Save a fact, document, or event to long-term archival memory.
    Use for things that don't need to be in active context.
    """
    if not MEMORY_MANAGER:
         return "Error: Memory System not initialized."
    return MEMORY_MANAGER.archival_memory_insert(content, tags)

@mcp.tool()
def archival_memory_search(query: str, limit: int = 5) -> str:
    """
    Search long-term archival memory using semantic search.
    """
    if not MEMORY_MANAGER:
         return "Error: Memory System not initialized."
    return MEMORY_MANAGER.archival_memory_search(query, limit)

@mcp.tool()
def get_core_memory_blocks() -> str:
    """
    Read all current Core Memory blocks.
    Useful to verify what you currently 'know' in your core memory.
    """
    if not MEMORY_MANAGER:
         return "Error: Memory System not initialized."
    
    blocks = MEMORY_MANAGER.agent_state.core_memory
    output = []
    for b in blocks:
        output.append(f"[{b.label}]\n{b.value}\n")
    return "\n".join(output)


# =============================================================================
# CRAWL4AI TOOL
# =============================================================================


@mcp.tool()
async def crawl_parallel(urls: list[str], session_dir: str) -> str:
    """
    High-speed parallel web scraping using crawl4ai.
    Scrapes multiple URLs concurrently, extracts clean markdown (removing ads/nav),
    and saves results to 'search_results' directory in the session workspace.

    Args:
        urls: List of URLs to scrape (no limit - crawl4ai handles parallel batches automatically)
        session_dir: Absolute path to the current session workspace (e.g. AGENT_RUN_WORKSPACES/session_...)

    Returns:
        JSON summary of results (success/fail counts, saved file paths).
    """
    try:
        from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
        from crawl4ai.content_filter_strategy import PruningContentFilter
        from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
    except ImportError:
        return json.dumps(
            {
                "error": "crawl4ai not installed. Run: uv pip install crawl4ai && crawl4ai-setup"
            }
        )

    import hashlib

    # 1. Configure Browser (Speed & Evasion)
    browser_config = BrowserConfig(
        headless=True,
        enable_stealth=True,
        browser_type="chromium",
    )

    # 2. Configure Extraction (Noise Reduction)
    # Pruning filter removes low-content density areas (ads, footers)
    prune_filter = PruningContentFilter(
        threshold=0.5, threshold_type="fixed", min_word_threshold=10
    )
    md_generator = DefaultMarkdownGenerator(content_filter=prune_filter)

    run_config = CrawlerRunConfig(
        markdown_generator=md_generator,
        excluded_tags=["nav", "footer", "header", "aside", "script", "style"],
        excluded_selector=".references, .footnotes, .citation, .bibliography, .ref-list, .endnotes, .field-name-field-footnotes, .field-name-field-reference, .cookie-banner, #cookie-consent, .eu-cookie-compliance, .donation, .donate, .subscription, .subscribe, .newsletter, .signup, .promo",
        cache_mode=CacheMode.BYPASS,  # Ensure fresh content
    )

    results_summary = {
        "total": len(urls),
        "successful": 0,
        "failed": 0,
        "saved_files": [],
        "errors": [],
    }

    search_results_dir = os.path.join(session_dir, "search_results")
    os.makedirs(search_results_dir, exist_ok=True)

    # 3. Execute Parallel Crawl
    try:
        async with AsyncWebCrawler(config=browser_config) as crawler:
            # arun_many uses a browser context pool for efficient parallelism
            results = await crawler.arun_many(urls=urls, config=run_config)

            for res in results:
                original_url = res.url
                if res.success:
                    # Generate filename from hash of URL to avoid length/char issues
                    url_hash = hashlib.md5(original_url.encode()).hexdigest()[:12]
                    filename = f"crawl_{url_hash}.md"
                    filepath = os.path.join(search_results_dir, filename)

                    # Prefer "fit_markdown" (filtered) over raw
                    content = res.markdown.fit_markdown or res.markdown.raw_markdown

                    # Add metadata header
                    final_content = f"# Source: {original_url}\n# Date: {datetime.utcnow().isoformat()}\n\n{content}"

                    with open(filepath, "w", encoding="utf-8") as f:
                        f.write(final_content)

                    results_summary["successful"] += 1
                    results_summary["saved_files"].append(
                        {
                            "url": original_url,
                            "file": filename,  # Relative name for brevity
                            "path": filepath,
                        }
                    )
                else:
                    results_summary["failed"] += 1
                    results_summary["errors"].append(
                        {"url": original_url, "error": res.error_message}
                    )

    except Exception as e:
        return json.dumps({"error": f"Crawl execution failed: {str(e)}"})

    return json.dumps(results_summary, indent=2)


# =============================================================================
# IMAGE GENERATION TOOLS
# =============================================================================

@mcp.tool()
def generate_image(
    prompt: str,
    input_image_path: str = None,
    output_dir: str = None,
    output_filename: str = None,
    preview: bool = False,
    model_name: str = "gemini-3-pro-image-preview"
) -> str:
    """
    Generate or edit an image using Gemini models.
    
    Args:
        prompt: Text description for generation, or edit instruction if input_image provided.
        input_image_path: Optional path to source image (for editing). If None, generates from scratch.
        output_dir: Directory to save output. Defaults to workspace work_products/media/.
        output_filename: Optional filename. If None, auto-generates with timestamp.
        preview: If True, launches Gradio viewer with the generated image.
        model_name: Gemini model to use. Defaults to "gemini-3-pro-image-preview".
        
    Returns:
        JSON with status, output_path, description, and viewer_url (if preview=True).
    """
    try:
        from google import genai
        from google.genai import types
        from google.genai.types import GenerateContentConfig, Part
        from PIL import Image
        import base64
        from io import BytesIO
        
        # Initialize Gemini client
        api_key = os.environ.get("GEMINI_API_KEY")
        if not api_key:
            return json.dumps({"error": "GEMINI_API_KEY not set in environment"})
        
        client = genai.Client(api_key=api_key)
        
        # Determine output directory
        if not output_dir:
            # Try to infer from workspace - look for work_products/media
            output_dir = os.path.join(os.getcwd(), "work_products", "media")
        
        os.makedirs(output_dir, exist_ok=True)
        
        # Prepare content for generation
        parts = []
        
        # If editing, include the input image
        if input_image_path:
            if not os.path.exists(input_image_path):
                return json.dumps({"error": f"Input image not found: {input_image_path}"})
            
            with open(input_image_path, "rb") as img_file:
                img_bytes = img_file.read()
                parts.append(Part.from_bytes(data=img_bytes, mime_type="image/png"))
        
        # Prepare request content
        parts.append(types.Part.from_text(text=prompt))
        content_obj = types.Content(role="user", parts=parts)
        
        # Generate the image using streaming (more robust for mixed modalities)
        response_stream = client.models.generate_content_stream(
            model=model_name,
            contents=[content_obj],
            config=GenerateContentConfig(
                response_modalities=["IMAGE", "TEXT"],
            )
        )
        
        saved_path = None
        text_output = ""
        
        for chunk in response_stream:
            if not chunk.candidates or not chunk.candidates[0].content or not chunk.candidates[0].content.parts:
                continue
                
            for part in chunk.candidates[0].content.parts:
                if hasattr(part, 'inline_data') and part.inline_data:
                    # Found image data
                    # Streaming API returns raw bytes in inline_data.data
                    image_data = part.inline_data.data
                    image = Image.open(BytesIO(image_data))
                    
                    # Generate filename if not provided
                    if not output_filename:
                        # Get description for filename
                        try:
                            description = describe_image_internal(image)
                        except:
                            description = "generated_image"
                            
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        safe_desc = "".join(c if c.isalnum() or c in (' ', '_') else '' for c in description)
                        safe_desc = "_".join(safe_desc.split()[:5])  # First 5 words
                        output_filename = f"{safe_desc}_{timestamp}.png"
                    
                    saved_path = os.path.join(output_dir, output_filename)
                    image.save(saved_path, "PNG")
                    
                elif hasattr(part, 'text') and part.text:
                    text_output += part.text

        if not saved_path:
            return json.dumps({"error": "No image generated", "text_output": text_output})
            
        result = {
            "success": True,
            "output_path": saved_path,
            "description": description if 'description' in locals() else None,
            "size_bytes": os.path.getsize(saved_path),
            "text_output": text_output if text_output else None
        }
        
        # Launch preview if requested
        if preview:
            try:
                viewer_result = preview_image(saved_path)
                viewer_data = json.loads(viewer_result)
                if "viewer_url" in viewer_data:
                    result["viewer_url"] = viewer_data["viewer_url"]
            except Exception as e:
                result["preview_error"] = str(e)
        
        return json.dumps(result, indent=2)
        
    except Exception as e:
        import traceback
        return json.dumps({"error": str(e), "traceback": traceback.format_exc()})


def describe_image_internal(image: 'Image.Image') -> str:
    """Internal helper to describe image without ZAI Vision (uses simple analysis)."""
    # Simple fallback description based on image properties
    width, height = image.size
    mode = image.mode
    return f"{mode}_image_{width}x{height}"


@mcp.tool()
def describe_image(image_path: str, max_words: int = 10) -> str:
    """
    Get a short description of an image using ZAI Vision (free).
    Useful for generating descriptive filenames.
    
    Args:
        image_path: Path to the image file.
        max_words: Maximum words in description (default 10).
        
    Returns:
        Short description suitable for filenames.
    """
    try:
        if not os.path.exists(image_path):
            return json.dumps({"error": f"Image not found: {image_path}"})
        
        # Try ZAI Vision via MCP if available
        try:
            # This would require calling the zai_vision MCP server
            # For now, fall back to simple description
            from PIL import Image
            img = Image.open(image_path)
            desc = describe_image_internal(img)
            return json.dumps({"description": desc})
        except Exception:
            # Fallback to basic file info
            filename = os.path.basename(image_path)
            return json.dumps({"description": f"image_{filename}"})
            
    except Exception as e:
        return json.dumps({"error": str(e)})


@mcp.tool()
def preview_image(image_path: str, port: int = 7860) -> str:
    """
    Open an image in the Gradio viewer for human review.
    Useful for viewing any existing image in the workspace.
    
    Args:
        image_path: Absolute path to the image file.
        port: Port to launch Gradio on (default 7860).
        
    Returns:
        JSON with viewer_url (e.g., "http://127.0.0.1:7860").
    """
    try:
        import subprocess
        
        if not os.path.exists(image_path):
            return json.dumps({"error": f"Image not found: {image_path}"})
        
        # Check if gradio_viewer.py script exists
        script_path = os.path.join(
            os.path.dirname(__file__), 
            "..", ".claude", "skills", "image-generation", "scripts", "gradio_viewer.py"
        )
        
        if not os.path.exists(script_path):
            return json.dumps({
                "error": "Gradio viewer script not found",
                "expected_path": script_path,
                "note": "Preview functionality requires image-generation skill to be initialized"
            })
        
        # Launch gradio viewer in background
        subprocess.Popen(
            [sys.executable, script_path, image_path, str(port)],
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL
        )
        
        viewer_url = f"http://127.0.0.1:{port}"
        return json.dumps({
            "success": True,
            "viewer_url": viewer_url,
            "image_path": image_path
        })
        
    except Exception as e:
        return json.dumps({"error": str(e)})


# =============================================================================
# MAIN - Start stdio server when run as a script
# =============================================================================

if __name__ == "__main__":
    # Run the MCP server using stdio transport
    mcp.run(transport="stdio")
