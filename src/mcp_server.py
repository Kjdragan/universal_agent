from mcp.server.fastmcp import FastMCP
from dotenv import load_dotenv
import os
import sys
import json
from datetime import datetime

# Ensure src path for imports
sys.path.append(os.path.abspath("src"))
# Ensure project root for Memory_System
sys.path.append(os.path.dirname(os.path.abspath(__file__))) # src/
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))) # Repo Root
from tools.workbench_bridge import WorkbenchBridge
from tools.workbench_bridge import WorkbenchBridge
from composio import Composio

# Memory System Integration
try:
    from Memory_System.manager import MemoryManager
    MEMORY_MANAGER = MemoryManager(storage_dir=os.path.join(os.path.dirname(__file__), "..", "Memory_System_Data"))
    sys.stderr.write("[Local Toolkit] Memory System initialized.\n")
except Exception as e:
    sys.stderr.write(f"[Local Toolkit] Memory System init failed: {e}\n")
    MEMORY_MANAGER = None

# Initialize Configuration
load_dotenv()

# Configure Logfire for MCP observability
try:
    import logfire
    if os.getenv("LOGFIRE_TOKEN"):
        logfire.configure(
            service_name="local-toolkit",
            send_to_logfire="if-token-present",
        )
        logfire.instrument_mcp()
        sys.stderr.write("[Local Toolkit] Logfire instrumentation enabled\\n")
except ImportError:
    pass

try:
    sys.stderr.write("[Local Toolkit] Server starting components...\n")
    mcp = FastMCP("Local Intelligence Toolkit")
except Exception:
    raise


def get_bridge():
    client = Composio(api_key=os.environ.get("COMPOSIO_API_KEY"))
    return WorkbenchBridge(composio_client=client, user_id="user_123")


def fix_path_typos(path: str) -> str:
    """
    Fix common model typos in workspace paths.
    Models sometimes truncate 'AGENT_RUN_WORKSPACES' to 'AGENT_RUNSPACES'.
    """
    # Fix: AGENT_RUNSPACES -> AGENT_RUN_WORKSPACES
    if "AGENT_RUNSPACES" in path and "AGENT_RUN_WORKSPACES" not in path:
        path = path.replace("AGENT_RUNSPACES", "AGENT_RUN_WORKSPACES")
        sys.stderr.write(f"[Local Toolkit] Path auto-corrected: AGENT_RUNSPACES â†’ AGENT_RUN_WORKSPACES\n")
    return path


@mcp.tool()
def workbench_download(
    remote_path: str, local_path: str, session_id: str = None
) -> str:
    """
    Download a file from the Remote Composio Workbench to the Local Workspace.
    """
    bridge = get_bridge()
    result = bridge.download(remote_path, local_path, session_id=session_id)
    if result.get("error"):
        return f"Error: {result['error']}"
    return f"Successfully downloaded {remote_path} to {local_path}. Local path: {result.get('local_path')}"


@mcp.tool()
def workbench_upload(local_path: str, remote_path: str, session_id: str = None) -> str:
    """
    Upload a file from the Local Workspace to the Remote Composio Workbench.
    """
    bridge = get_bridge()
    result = bridge.upload(local_path, remote_path, session_id=session_id)
    if result.get("error"):
        return f"Error: {result['error']}"
    return f"Successfully uploaded {local_path} to {remote_path}."


@mcp.tool()
def read_local_file(path: str) -> str:
    """
    Read content from a file in the Local Workspace.
    """
    try:
        path = fix_path_typos(path)  # Auto-correct common model typos
        abs_path = os.path.abspath(path)
        if not os.path.exists(abs_path):
            return f"Error: File not found at {path}"

        with open(abs_path, "r", encoding="utf-8") as f:
            content = f.read()
        return content
    except Exception as e:
        return f"Error reading file: {str(e)}"


@mcp.tool()
def write_local_file(path: str, content: str) -> str:
    """
    Write content to a file in the Local Workspace.
    Useful for saving reports, summaries, or code generated by sub-agents.
    """
    try:
        abs_path = os.path.abspath(path)
        os.makedirs(os.path.dirname(abs_path), exist_ok=True)
        with open(abs_path, "w", encoding="utf-8") as f:
            f.write(content)
        return f"Successfully wrote {len(content)} chars to {path}"
    except Exception as e:
        return f"Error writing file: {str(e)}"


@mcp.tool()
def list_directory(path: str) -> str:
    """
    List contents of a directory in the Local Workspace.
    """
    try:
        path = fix_path_typos(path)  # Auto-correct common model typos
        abs_path = os.path.abspath(path)
        if not os.path.exists(abs_path):
            return f"Error: Directory not found at {path}"
        
        items = os.listdir(abs_path)
        return json.dumps(items, indent=2)
    except Exception as e:
        return f"Error listing directory: {str(e)}"



@mcp.tool()
def compress_files(files: list[str], output_archive: str) -> str:
    """
    Compress a list of files into a zip archive.
    Args:
        files: List of absolute file paths to include.
        output_archive: Absolute path for the output zip file.
    """
    import zipfile
    try:
        # Validate input paths
        validated_files = []
        for f in files:
            abs_path = os.path.abspath(f)
            if not os.path.exists(abs_path):
                return json.dumps({"error": f"File not found: {f}"})
            validated_files.append(abs_path)

        output_path = os.path.abspath(output_archive)
        
        # Create parent directory if needed
        os.makedirs(os.path.dirname(output_path), exist_ok=True)

        with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zf:
            for file_path in validated_files:
                # Arcname is the name inside the zip file (basename)
                zf.write(file_path, arcname=os.path.basename(file_path))
                
        # Check if created
        if os.path.exists(output_path):
             size = os.path.getsize(output_path)
             return json.dumps({
                 "success": True, 
                 "archive_path": output_path,
                 "size_bytes": size,
                 "files_included": len(validated_files)
             })
        return json.dumps({"error": "Failed to create archive file"})

    except Exception as e:
        return json.dumps({"error": f"Compression failed: {str(e)}"})


@mcp.tool()
def upload_to_composio(
    path: str, 
    tool_slug: str = "GMAIL_SEND_EMAIL",
    toolkit_slug: str = "gmail"
) -> str:
    """
    Upload a local file to Composio S3 for use as an email attachment or other tool input.
    Uses native Composio SDK FileUploadable.from_path() - the correct, supported method.
    
    Args:
        path: Absolute path to the local file to upload
        tool_slug: The Composio tool that will consume this file (default: GMAIL_SEND_EMAIL)
        toolkit_slug: The toolkit the tool belongs to (default: gmail)
    
    Returns JSON with:
    - s3key: ID for tool attachments (pass to Gmail/Slack)
    - mimetype: Detected file type
    - name: Original filename
    """
    try:
        abs_path = os.path.abspath(path)
        if not os.path.exists(abs_path):
            return json.dumps({"error": f"File not found: {path}"})
        
        # Import native Composio file helper
        from composio.core.models._files import FileUploadable
        
        # Get Composio client
        client = Composio(api_key=os.environ.get("COMPOSIO_API_KEY"))
        
        # Use native SDK method - this is the correct approach per Composio docs
        sys.stderr.write(f"[upload_to_composio] Uploading {abs_path} via native FileUploadable.from_path()\\n")
        
        result = FileUploadable.from_path(
            client=client.client,
            file=abs_path,
            tool=tool_slug,
            toolkit=toolkit_slug
        )
        
        # Return the attachment-ready format
        response = {
            "s3key": result.s3key,
            "mimetype": result.mimetype,
            "name": result.name,
            "local_path": abs_path
        }
        
        sys.stderr.write(f"[upload_to_composio] SUCCESS: s3key={result.s3key}\\n")
        return json.dumps(response, indent=2)
        
    except Exception as e:
        import traceback
        sys.stderr.write(f"[upload_to_composio] ERROR: {traceback.format_exc()}\\n")
        return json.dumps({"error": str(e)})

# =============================================================================
# MEMORY SYSTEM TOOLS
# =============================================================================

@mcp.tool()
def core_memory_replace(label: str, new_value: str) -> str:
    """
    Overwrite a Core Memory block (e.g. 'human', 'persona').
    Use this to update persistent facts about the user or yourself.
    """
    if not MEMORY_MANAGER:
        return "Error: Memory System not initialized."
    return MEMORY_MANAGER.core_memory_replace(label, new_value)

@mcp.tool()
def core_memory_append(label: str, text_to_append: str) -> str:
    """
    Append text to a Core Memory block.
    Useful for adding a new preference without deleting old ones.
    """
    if not MEMORY_MANAGER:
         return "Error: Memory System not initialized."
    return MEMORY_MANAGER.core_memory_append(label, text_to_append)

@mcp.tool()
def archival_memory_insert(content: str, tags: str = "") -> str:
    """
    Save a fact, document, or event to long-term archival memory.
    Use for things that don't need to be in active context.
    """
    if not MEMORY_MANAGER:
         return "Error: Memory System not initialized."
    return MEMORY_MANAGER.archival_memory_insert(content, tags)

@mcp.tool()
def archival_memory_search(query: str, limit: int = 5) -> str:
    """
    Search long-term archival memory using semantic search.
    """
    if not MEMORY_MANAGER:
         return "Error: Memory System not initialized."
    return MEMORY_MANAGER.archival_memory_search(query, limit)

@mcp.tool()
def get_core_memory_blocks() -> str:
    """
    Read all current Core Memory blocks.
    Useful to verify what you currently 'know' in your core memory.
    """
    if not MEMORY_MANAGER:
         return "Error: Memory System not initialized."
    
    blocks = MEMORY_MANAGER.agent_state.core_memory
    output = []
    for b in blocks:
        output.append(f"[{b.label}]\n{b.value}\n")
    return "\n".join(output)


# =============================================================================
# CRAWL4AI TOOL
# =============================================================================


@mcp.tool()
async def crawl_parallel(urls: list[str], session_dir: str) -> str:
    """
    High-speed parallel web scraping using crawl4ai.
    Scrapes multiple URLs concurrently, extracts clean markdown (removing ads/nav),
    and saves results to 'search_results' directory in the session workspace.

    Args:
        urls: List of URLs to scrape (no limit - crawl4ai handles parallel batches automatically)
        session_dir: Absolute path to the current session workspace (e.g. AGENT_RUN_WORKSPACES/session_...)

    Returns:
        JSON summary of results (success/fail counts, saved file paths).
    """
    try:
        from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
        from crawl4ai.content_filter_strategy import PruningContentFilter
        from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
    except ImportError:
        return json.dumps(
            {
                "error": "crawl4ai not installed. Run: uv pip install crawl4ai && crawl4ai-setup"
            }
        )

    import hashlib

    # 1. Configure Browser (Speed & Evasion)
    browser_config = BrowserConfig(
        headless=True,
        enable_stealth=True,
        browser_type="chromium",
    )

    # 2. Configure Extraction (Noise Reduction)
    # Pruning filter removes low-content density areas (ads, footers)
    prune_filter = PruningContentFilter(
        threshold=0.5, threshold_type="fixed", min_word_threshold=10
    )
    md_generator = DefaultMarkdownGenerator(content_filter=prune_filter)

    run_config = CrawlerRunConfig(
        markdown_generator=md_generator,
        excluded_tags=["nav", "footer", "header", "aside", "script", "style"],
        excluded_selector=".references, .footnotes, .citation, .bibliography, .ref-list, .endnotes, .field-name-field-footnotes, .field-name-field-reference, .cookie-banner, #cookie-consent, .eu-cookie-compliance, .donation, .donate, .subscription, .subscribe, .newsletter, .signup, .promo",
        cache_mode=CacheMode.BYPASS,  # Ensure fresh content
    )

    results_summary = {
        "total": len(urls),
        "successful": 0,
        "failed": 0,
        "saved_files": [],
        "errors": [],
    }

    search_results_dir = os.path.join(session_dir, "search_results")
    os.makedirs(search_results_dir, exist_ok=True)

    # 3. Execute Parallel Crawl
    try:
        async with AsyncWebCrawler(config=browser_config) as crawler:
            # arun_many uses a browser context pool for efficient parallelism
            results = await crawler.arun_many(urls=urls, config=run_config)

            for res in results:
                original_url = res.url
                if res.success:
                    # Generate filename from hash of URL to avoid length/char issues
                    url_hash = hashlib.md5(original_url.encode()).hexdigest()[:12]
                    filename = f"crawl_{url_hash}.md"
                    filepath = os.path.join(search_results_dir, filename)

                    # Prefer "fit_markdown" (filtered) over raw
                    content = res.markdown.fit_markdown or res.markdown.raw_markdown

                    # Add metadata header
                    final_content = f"# Source: {original_url}\n# Date: {datetime.utcnow().isoformat()}\n\n{content}"

                    with open(filepath, "w", encoding="utf-8") as f:
                        f.write(final_content)

                    results_summary["successful"] += 1
                    results_summary["saved_files"].append(
                        {
                            "url": original_url,
                            "file": filename,  # Relative name for brevity
                            "path": filepath,
                        }
                    )
                else:
                    results_summary["failed"] += 1
                    results_summary["errors"].append(
                        {"url": original_url, "error": res.error_message}
                    )

    except Exception as e:
        return json.dumps({"error": f"Crawl execution failed: {str(e)}"})

    return json.dumps(results_summary, indent=2)


# =============================================================================
# MAIN - Start stdio server when run as a script
# =============================================================================

if __name__ == "__main__":
    # Run the MCP server using stdio transport
    mcp.run(transport="stdio")
