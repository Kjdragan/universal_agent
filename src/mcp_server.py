from mcp.server.fastmcp import FastMCP
from dotenv import load_dotenv
import os
import sys
import json
from datetime import datetime

# Ensure src path for imports
sys.path.append(os.path.abspath("src"))
from tools.workbench_bridge import WorkbenchBridge
from composio import Composio

# Initialize Configuration
load_dotenv()

try:
    sys.stderr.write("[Local Toolkit] Server starting components...\n")
    mcp = FastMCP("Local Intelligence Toolkit")
except Exception:
    raise


def get_bridge():
    client = Composio(api_key=os.environ.get("COMPOSIO_API_KEY"))
    return WorkbenchBridge(composio_client=client, user_id="user_123")


@mcp.tool()
def workbench_download(
    remote_path: str, local_path: str, session_id: str = None
) -> str:
    """
    Download a file from the Remote Composio Workbench to the Local Workspace.
    """
    bridge = get_bridge()
    result = bridge.download(remote_path, local_path, session_id=session_id)
    if result.get("error"):
        return f"Error: {result['error']}"
    return f"Successfully downloaded {remote_path} to {local_path}. Local path: {result.get('local_path')}"


@mcp.tool()
def workbench_upload(local_path: str, remote_path: str, session_id: str = None) -> str:
    """
    Upload a file from the Local Workspace to the Remote Composio Workbench.
    """
    bridge = get_bridge()
    result = bridge.upload(local_path, remote_path, session_id=session_id)
    if result.get("error"):
        return f"Error: {result['error']}"
    return f"Successfully uploaded {local_path} to {remote_path}."


@mcp.tool()
def read_local_file(path: str) -> str:
    """
    Read content from a file in the Local Workspace.
    """
    try:
        abs_path = os.path.abspath(path)
        if not os.path.exists(abs_path):
            return f"Error: File not found at {path}"

        with open(abs_path, "r", encoding="utf-8") as f:
            content = f.read()
        return content
    except Exception as e:
        return f"Error reading file: {str(e)}"


@mcp.tool()
def write_local_file(path: str, content: str) -> str:
    """
    Write content to a file in the Local Workspace.
    Useful for saving reports, summaries, or code generated by sub-agents.
    """
    try:
        abs_path = os.path.abspath(path)
        os.makedirs(os.path.dirname(abs_path), exist_ok=True)
        with open(abs_path, "w", encoding="utf-8") as f:
            f.write(content)
        return f"Successfully wrote {len(content)} chars to {path}"
    except Exception as e:
        return f"Error writing file: {str(e)}"


@mcp.tool()
def list_directory(path: str) -> str:
    """
    List contents of a directory in the Local Workspace.
    """
    try:
        abs_path = os.path.abspath(path)
        if not os.path.exists(abs_path):
            return f"Error: Directory not found at {path}"
        
        items = os.listdir(abs_path)
        return json.dumps(items, indent=2)
    except Exception as e:
        return f"Error listing directory: {str(e)}"



@mcp.tool()
def upload_to_composio(path: str, session_id: str = None) -> str:
    """
    "Teleport" a local file to Composio's Cloud environment (Remote Workbench + S3).
    Use this to stage files for Email Attachments, Slack Uploads, etc.
    
    Returns JSON with:
    - s3_key: ID for tool attachments (e.g. Gmail)
    - s3_url: Direct download link
    - remote_path: Path in the workbench container
    - mimetype: Detected file type
    """
    try:
        abs_path = os.path.abspath(path)
        if not os.path.exists(abs_path):
            return json.dumps({"error": f"File not found: {path}"})
            
        filename = os.path.basename(abs_path)
        remote_path = f"/home/user/{filename}"
        
        # 1. Bridge: Upload Local -> Remote Workbench
        bridge = get_bridge()
        upload_res = bridge.upload(abs_path, remote_path, session_id=session_id)
        
        if not upload_res.get("success"):
            return json.dumps({"error": f"Bridge Upload Failed: {upload_res.get('error')}"})
            
        # 2. S3 Staging: Execute 'upload_local_file' on Remote Workbench
        # This helper function is pre-loaded in the workbench environment
        script = (
            f"res, err = upload_local_file('{remote_path}')\n"
            "if err: print(f'S3_ERROR: {err}')\n"
            "else: \n"
            "    import json\n"
            "    print(json.dumps(res))"
        )
        
        client = bridge.client
        s3_resp = client.tools.execute(
            slug="COMPOSIO_REMOTE_WORKBENCH", 
            arguments={"code_to_execute": script, "session_id": session_id} if session_id else {"code_to_execute": script},
            user_id=bridge.user_id,
            dangerously_skip_version_check=True
        )
        
        # Parse S3 Result
        stdout = ""
        if hasattr(s3_resp, "data"):
             stdout = s3_resp.data.get("stdout", "")
        elif isinstance(s3_resp, dict):
             stdout = s3_resp.get("data", {}).get("stdout", "")
             
        if "S3_ERROR" in stdout or not stdout.strip():
            return json.dumps({"error": f"S3 Staging Failed: {stdout}"})
            
        # Extract JSON from stdout (might be mixed with other logs, find the last JSON object)
        try:
            # Simple heuristic: Split lines, find the one that looks like JSON
            for line in stdout.splitlines():
                if '"s3key":' in line:
                    s3_data = json.loads(line)
                    s3_data["remote_path"] = remote_path
                    return json.dumps(s3_data, indent=2)
        except Exception as e:
            return json.dumps({"error": f"Failed to parse S3 response: {stdout}. Error: {e}"})

        return json.dumps({"error": f"No valid JSON found in S3 response: {stdout}"})

    except Exception as e:
        return json.dumps({"error": str(e)})

# =============================================================================
# CRAWL4AI TOOL
# =============================================================================


@mcp.tool()
async def crawl_parallel(urls: list[str], session_dir: str) -> str:
    """
    High-speed parallel web scraping using crawl4ai.
    Scrapes multiple URLs concurrently, extracts clean markdown (removing ads/nav),
    and saves results to 'search_results' directory in the session workspace.

    Args:
        urls: List of URLs to scrape (no limit - crawl4ai handles parallel batches automatically)
        session_dir: Absolute path to the current session workspace (e.g. AGENT_RUN_WORKSPACES/session_...)

    Returns:
        JSON summary of results (success/fail counts, saved file paths).
    """
    try:
        from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
        from crawl4ai.content_filter_strategy import PruningContentFilter
        from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
    except ImportError:
        return json.dumps(
            {
                "error": "crawl4ai not installed. Run: uv pip install crawl4ai && crawl4ai-setup"
            }
        )

    import hashlib

    # 1. Configure Browser (Speed & Evasion)
    browser_config = BrowserConfig(
        headless=True,
        enable_stealth=True,
        browser_type="chromium",
    )

    # 2. Configure Extraction (Noise Reduction)
    # Pruning filter removes low-content density areas (ads, footers)
    prune_filter = PruningContentFilter(
        threshold=0.5, threshold_type="fixed", min_word_threshold=10
    )
    md_generator = DefaultMarkdownGenerator(content_filter=prune_filter)

    run_config = CrawlerRunConfig(
        markdown_generator=md_generator,
        excluded_tags=["nav", "footer", "header", "aside", "script", "style"],
        excluded_selector=".references, .footnotes, .citation, .bibliography, .ref-list, .endnotes, .field-name-field-footnotes, .field-name-field-reference, .cookie-banner, #cookie-consent, .eu-cookie-compliance, .donation, .donate, .subscription, .subscribe, .newsletter, .signup, .promo",
        cache_mode=CacheMode.BYPASS,  # Ensure fresh content
    )

    results_summary = {
        "total": len(urls),
        "successful": 0,
        "failed": 0,
        "saved_files": [],
        "errors": [],
    }

    search_results_dir = os.path.join(session_dir, "search_results")
    os.makedirs(search_results_dir, exist_ok=True)

    # 3. Execute Parallel Crawl
    try:
        async with AsyncWebCrawler(config=browser_config) as crawler:
            # arun_many uses a browser context pool for efficient parallelism
            results = await crawler.arun_many(urls=urls, config=run_config)

            for res in results:
                original_url = res.url
                if res.success:
                    # Generate filename from hash of URL to avoid length/char issues
                    url_hash = hashlib.md5(original_url.encode()).hexdigest()[:12]
                    filename = f"crawl_{url_hash}.md"
                    filepath = os.path.join(search_results_dir, filename)

                    # Prefer "fit_markdown" (filtered) over raw
                    content = res.markdown.fit_markdown or res.markdown.raw_markdown

                    # Add metadata header
                    final_content = f"# Source: {original_url}\n# Date: {datetime.utcnow().isoformat()}\n\n{content}"

                    with open(filepath, "w", encoding="utf-8") as f:
                        f.write(final_content)

                    results_summary["successful"] += 1
                    results_summary["saved_files"].append(
                        {
                            "url": original_url,
                            "file": filename,  # Relative name for brevity
                            "path": filepath,
                        }
                    )
                else:
                    results_summary["failed"] += 1
                    results_summary["errors"].append(
                        {"url": original_url, "error": res.error_message}
                    )

    except Exception as e:
        return json.dumps({"error": f"Crawl execution failed: {str(e)}"})

    return json.dumps(results_summary, indent=2)


# =============================================================================
# MAIN - Start stdio server when run as a script
# =============================================================================

if __name__ == "__main__":
    # Run the MCP server using stdio transport
    mcp.run(transport="stdio")
