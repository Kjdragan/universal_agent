# Product Requirements Document (PRD): The Universal Agent Corporation

## 1. Project Overview & Objective

The "Universal Agent Corporation" is an architectural expansion from a monolithic Primary/Worker model into a **Symmetrical Distributed Factory** model.

The goal is to enable the exact same Universal Agent codebase to be deployed across multiple environments (e.g., a VPS "Headquarters", and multiple Local Desktop "Branch Factories"). Each deployment will possess a complete suite of agentic capabilities (including autonomous VP Coders), but their behavior, permissions, and available resources will be strictly controlled via dynamic parameterization (`.env` toggles).

## 2. Core Requirements

### 2.1 Symmetrical, Parameterized Factories

**Requirement:** The codebase must remain identical across all deployments.

- **Factory Roles:**
  - **Headquarters (VPS):** Acts as the primary orchestrator (`IS_HEADQUARTERS=True`). It receives external inputs (Telegram), handles primary job orchestration, and delegates tasks when necessary.
  - **Branch Factories (Local):** Act as auxiliary nodes and local execution environments (`IS_HEADQUARTERS=False`).
- **VP Sub-Agents:** Every factory inherently possesses the ability to run all VP Agents (e.g., `VP_CODER`, `VP_GENERAL`).
  - A Local Factory can utilize its own autonomous `VP_CODER` to execute a task routed to it by Headquarters.
  - Features, agents, and capabilities can be aggressively pruned via parameterization (e.g., `ENABLE_VP_CODER=False`).

### 2.2 Independent LLM Inference Configurations

**Requirement:** Factories must be able to utilize distinct, configurable inference providers.

- **Why:** To manage costs and specialize compute. Headquarters may run on a premium ZAI subscription or OpenAI account, while a Local Factory handling low-tier background tasks runs on a secondary ZAI account, or a local/cheaper provider like Gemini Flash.
- **Implementation:** The Local Factory parameterization must dynamically respect our `.env` routing strategy. Specifically, because ZAI frequently emulates Anthropic endpoints, the payload must correctly route to the factory's localized `ANTHROPIC_BASE_URL` (e.g., `https://api.z.ai/api/anthropic`) and `ANTHROPIC_API_KEY`, alongside standard identifiers like `OPENAI_BASE_URL` and `MODEL_NAME`. This guarantees that factory nodes can drop-in substitute alternative ZAI or direct provider credentials gracefully.

### 2.3 The Communication Layer (The Message Bus)

**Requirement:** Headquarters and Branch Factories must communicate via a fast, reliable, and cost-effective distributed message queue.

- **Recommendation:** **Redis Streams.**
  - **Pros:** Zero additional infrastructure cost (can simply run as a Docker container on the existing Hostinger VPS alongside the Postgres database). Exceptionally fast, supports consumer groups (if you add multiple local desktops later), and handles "Stateless Delegations" perfectly. Headquarters publishes a json payload to a Redis Stream; a Local Factory pulls it, executes it using its local VP Coder, and pushes the result back.
  
### 2.4 Hierarchical Organization & Personas

**Requirement:** The corporate hierarchy relies on distinct identities so the user always knows which factory is responding, and which sub-agent is evaluating tasks.

- **The VPS Headquarters (HQ) Factory:**
  - **HQ Primary:** Simone
  - **HQ VP Coder:** Codie
  - **HQ VP General:** HAL
- **The Local Desktop Factory:**
  - **Local Primary:** Homer
  - **Local VP Coder:** Lisa
  - **Local VP General:** Bart

**Command Flow:**

- The CEO (The User) directs Headquarters Simone.
- HQ Simone evaluates the task. If it requires global orchestration, HQ Simone delegates it to HQ VP Coder (Codie) or HQ VP General (HAL).
- If the task requires localized execution (e.g., writing to a local repository), HQ Simone publishes the task to the Redis message bus.
- The Local Primary (Homer) pulls the task, and delegates it to Local VP Coder (Lisa) or Local VP General (Bart) to execute.
- **Human Headless Interventions:** The User should *not* routinely interact directly with Homer (Local Primary). However, if Headquarters acts as a single point of failure and goes offline, the User must retain fall-back capabilities to address Homer directly.

### 2.5 Observability & UX

**Requirement:** The Headquarters Web UI must provide a "Corporation View".

- **Real-Time Fleet Status:** A new dashboard tab must visualize all connected factories, displaying their heartbeat latency, current active `.env` capabilities (`VP_CODER: Enabled`), and active tasks.
- **Cost Tracking:** The dashboard should aggregate inference costs (ZAI telemetry) across all participating factories.

## 3. The Implementation Strategy (Concurrent Evergreen)

The rollout of the Corporation Architecture must not block ongoing maintenance and feature development of the current Headquarters deployment.

**Phase 1: Evergreen Headquarters (Ongoing)**

- Continue resolving bugs, enhancing the existing VP orchestrations, and improving the Web UI on the VPS.

**Phase 2: Foundation & Parameterization**

- Introduce the strict `.env` parameters (`FACTORY_ROLE`, `ENABLE_VP_CODER`, `LLM_PROVIDER`) to the codebase.
- Implement logic so that agents dynamically read these capabilities on boot (e.g., generating `capabilities.md` dynamically based on the `.env`).

**Phase 3: Redis Infrastructure & Stateless Delegation**

- Deploy a Redis container on the Hostinger VPS.
- Replace point-to-point bridging scripts (like the `tutorial_local_bootstrap_worker.py`) with generalized Redis Stream consumers in the Universal Agent core.

**Phase 4: Corporation Observability UX**

- Build the "Corporation View" fleet dashboard in the Next.js UI.
- Implement the "Memo Promotion" memory synchronization logic (Local Factory summarizes its session logs and promotes valuable insights up to Headquarters).

## 4. Security Considerations & Secrets Management

Managing plain-text `.env` files across multiple factories (e.g., `hq.env`, `factory_1.env`) manually is error-prone, insecure, and leads to configuration drift.

**Recommendation: A Centralized Secrets Manager**
We should adopt a developer-friendly secrets manager platform. The leading options for universal agent ecosystems are **Infisical** or **Doppler**.

- **How it works:** Instead of maintaining a `.env` file, the Universal Agent backend boots up and queries the Secrets Manager API.
- **Environment Roles:** You define "Environments" in their cloud dashboard (e.g., `HQ_Production`, `Local_Desktop_Homer`).
- **Shared vs. Scoped Secrets:** You can define a shared key once (e.g., `COMPOSIO_API_KEY`) and it syncs to all factories automatically. However, you explicitly isolate high-risk keys (like `POSTGRES_DB_PASSWORD`) so *only* the `HQ_Production` environment can pull it. Local Factories simply won't receive the key during boot.
- **Implementation:** Both platforms provide pre-built Python SDKs and CLI tools that map directly into our existing `pydantic-settings` or `python-dotenv` loaders, meaning virtually no code rewrites are required.

- **Temporary Ops Tokens:** Even with a secrets manager, Local Factories execute tasks via short-lived Ops Tokens generated by HQ rather than holding permanent root credentials.
- **Sandboxed Execution:** Local Factory capabilities are bound strictly by their parameters, acting as a defense-in-depth measure against rogue LLM execution.
- **Credential Exposure Protocol:** If any Machine Identity or personal auth token is ever shared in chat logs, scripts, or commits, rotate it immediately in Infisical and update all affected factory environments before the next deploy.
