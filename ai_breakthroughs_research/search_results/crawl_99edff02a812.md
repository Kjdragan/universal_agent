---
title: "[2408.05211] VITA: Towards Open-Source Interactive Omni Multimodal LLM"
source: https://arxiv.org/abs/2408.05211
date: 2024-08-09
description: "Abstract page for arXiv paper 2408.05211: VITA: Towards Open-Source Interactive Omni Multimodal LLM"
word_count: 715
---

# Computer Science > Computer Vision and Pattern Recognition
**arXiv:2408.05211** (cs) 
Submitted on 9 Aug 2024 ([v1), last revised 30 May 2025 (this version, v3)]
# Title:VITA: Towards Open-Source Interactive Omni Multimodal LLM
Authors:Chaoyou Fu, Haojia Lin, Zuwei Long, Yunhang Shen, Yuhang Dai, Meng Zhao, Yi-Fan Zhang, Shaoqi Dong, Yangze Li, Xiong Wang, Haoyu Cao, Di Yin, Long Ma, Xiawu Zheng, Rongrong Ji, Yunsheng Wu, Ran He, Caifeng Shan, Xing Sun
View a PDF of the paper titled VITA: Towards Open-Source Interactive Omni Multimodal LLM, by Chaoyou Fu and 18 other authors
View PDF HTML (experimental)
> Abstract:The remarkable multimodal capabilities and interactive experience of GPT-4o underscore their necessity in practical applications, yet open-source models rarely excel in both areas. In this paper, we introduce VITA, the first-ever open-source Multimodal Large Language Model (MLLM) adept at simultaneous processing and analysis of Video, Image, Text, and Audio modalities, and meanwhile has an advanced multimodal interactive experience. Starting from Mixtral 8x7B as a language foundation, we expand its Chinese vocabulary followed by bilingual instruction tuning. We further endow the language model with visual and audio capabilities through two-stage multi-task learning of multimodal alignment and instruction tuning. VITA demonstrates robust foundational capabilities of multilingual, vision, and audio understanding, as evidenced by its strong performance across a range of both unimodal and multimodal benchmarks. Beyond foundational capabilities, we have made considerable progress in enhancing the natural multimodal human-computer interaction experience. VITA is the first step for the open-source community to explore the seamless integration of multimodal understanding and interaction. While there is still lots of work to be done on VITA to get close to close-source counterparts, we hope that its role as a pioneer can serve as a cornerstone for subsequent research. Project Page: this https URL. 
Comments: | Project Page: this https URL  
---|---  
Subjects: |  Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)  
Cite as: | arXiv:2408.05211 [cs.CV]  
(or  arXiv:2408.05211v3 [cs.CV] for this version)   
<https://doi.org/10.48550/arXiv.2408.05211> Focus to learn more arXiv-issued DOI via DataCite  
## Submission history
From: Chaoyou Fu [view email] **[[v1]](https://arxiv.org/abs/</abs/2408.05211v1>)** Fri, 9 Aug 2024 17:59:49 UTC (831 KB) **[[v2]](https://arxiv.org/abs/</abs/2408.05211v2>)** Tue, 10 Sep 2024 13:21:08 UTC (965 KB) **[v3]** Fri, 30 May 2025 12:57:16 UTC (739 KB) 
Full-text links:
## Access Paper:
View a PDF of the paper titled VITA: Towards Open-Source Interactive Omni Multimodal LLM, by Chaoyou Fu and 18 other authors
  * View PDF
  * HTML (experimental)
  * TeX Source 

view license
Current browse context: 
cs.CV
< prev") |  next >")
new |  recent | 2024-08
Change to browse by: 
cs cs.AI cs.CL
### References & Citations
  * NASA ADS
  * Google Scholar
  * Semantic Scholar

export BibTeX citation Loading...
## BibTeX formatted citation
Ã—
loading...
Data provided by: 
### Bookmark
   
Bibliographic Tools
# Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer _(What is the Explorer?)_
Connected Papers Toggle
Connected Papers _(What is Connected Papers?)_
Litmaps Toggle
Litmaps _(What is Litmaps?)_
scite.ai Toggle
scite Smart Citations _(What are Smart Citations?)_
Code, Data, Media
# Code, Data and Media Associated with this Article
alphaXiv Toggle
alphaXiv _(What is alphaXiv?)_
Links to Code Toggle
CatalyzeX Code Finder for Papers _(What is CatalyzeX?)_
DagsHub Toggle
DagsHub _(What is DagsHub?)_
GotitPub Toggle
Gotit.pub _(What is GotitPub?)_
Huggingface Toggle
Hugging Face _(What is Huggingface?)_
Links to Code Toggle
Papers with Code _(What is Papers with Code?)_
ScienceCast Toggle
ScienceCast _(What is ScienceCast?)_
Demos
# Demos
Replicate Toggle
Replicate _(What is Replicate?)_
Spaces Toggle
Hugging Face Spaces _(What is Spaces?)_
Spaces Toggle
TXYZ.AI _(What is TXYZ.AI?)_
Related Papers
# Recommenders and Search Tools
Link to Influence Flower
Influence Flower _(What are Influence Flowers?)_
Core recommender toggle
CORE Recommender _(What is CORE?)_
  * Author
  * Venue
  * Institution
  * Topic

About arXivLabs 
# arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? **Learn more about arXivLabs**.
Which authors of this paper are endorsers? | Disable MathJax>) (What is MathJax?) 
