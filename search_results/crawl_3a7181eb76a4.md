---
title: "[en.filters.with_topics] - Hugging Face Forums"
source: https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/150882
date: unknown
description: "Hey everyone, 
I‚Äôm planning to fine-tune a 70B parameter model like LLaMA 3.1 locally. I know it needs around 280GB VRAM for the model weights alone, and more for gradients/activations. With a 16GB VR"
word_count: 666
---

[ [en.skip_to_main_content] ](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/<#main-container>)
  *  Topics  
  * [en.sidebar.more] 

[en.sidebar.sections.categories.header_link_text] 
  *  Beginners  
  *  Intermediate  
  *  Course  
  *  Research  
  *  Models  
  * [ [en.sidebar.all_categories]  ](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</categories>)

‚Äã 
‚Äã 
You can login using your huggingface.co credentials.
This forum is powered by Discourse and relies on a trust-level system. As a new user, you‚Äôre temporarily limited in the number of topics and posts you can create. To lift those restrictions, just spend time reading other posts (to be precise, enter 5 topics, read through 30 posts and spend a total of 10 minutes reading).
Start with reading this post. Then maybe someone already had that error that is bugging you check with a quick search. Or you can read the latest awesome paper the team discussed.
# [en.discovery.headings.all.default]
  1. [en.categories.categories_label] 

  * [ [en.filters.hot.title count=0] ](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</hot>)
  * [ [en.filters.latest.title count=0] ](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</latest>)
  * [ [en.filters.categories.title count=0] ](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</categories>)
  * [ [en.filters.top.title count=0] ](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</top>)

[en.sr_topic_list_caption] [en.topic.title]  |  [en.posters]  |  [en.replies]  |  [en.views]  |  [en.activity]   
---|---|---|---|---  
Do AI models feel? Research |            |   91  |  [en.number.short.thousands] |  [[en.dates.tiny.x_minutes count=9]](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</t/do-ai-models-feel/170991/101>)  
Thought Filtering vs. Text Filtering: Empirical Evidence of Latent Space Defense Supremacy Against Adversarial Obfuscation Research |      |   2  |  24 |  [[en.dates.tiny.about_x_hours count=1]](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</t/thought-filtering-vs-text-filtering-empirical-evidence-of-latent-space-defense-supremacy-against-adversarial-obfuscation/172453/3>)  
A Bidirectional LLM Firewall: Next Level X1 - help wanted! Models |          |   17  |  141 |  [[en.dates.tiny.about_x_hours count=1]](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</t/a-bidirectional-llm-firewall-next-level-x1-help-wanted/172352/18>)  
DetLLM ‚Äì Deterministic Inference Checks ü§óTransformers |    |   0  |  8 |  [[en.dates.tiny.about_x_hours count=2]](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</t/detllm-deterministic-inference-checks/172541/1>)  
Error when trying to dowload a model in Ollama Beginners |    |   0  |  8 |  [[en.dates.tiny.about_x_hours count=3]](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</t/error-when-trying-to-dowload-a-model-in-ollama/172540/1>)  
How can I generate the exact same image twice using AI image generation tools? Beginners |            |   5  |  [en.number.short.thousands] |  [[en.dates.tiny.about_x_hours count=8]](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</t/how-can-i-generate-the-exact-same-image-twice-using-ai-image-generation-tools/127913/6>)  
AERIS V20 ‚Äì Architectural Constraints for Non-Standard LLM Behavior Research |        |   5  |  76 |  [[en.dates.tiny.about_x_hours count=9]](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</t/aeris-v20-architectural-constraints-for-non-standard-llm-behavior/172071/6>)  
Trad FR - Issue 44 - Science for all Beginners |    |   0  |  7 |  [[en.dates.tiny.about_x_hours count=11]](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</t/trad-fr-issue-44-science-for-all/172537/1>)  
Repetitive Answers From Fine-Tuned LLM Models |            |   12  |  [en.number.short.thousands] |  [[en.dates.tiny.about_x_hours count=17]](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</t/repetitive-answers-from-fine-tuned-llm/110024/14>)  
Training cross-encoders Intermediate |      |   1  |  8 |  [[en.dates.tiny.about_x_hours count=18]](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</t/training-cross-encoders/172531/2>)  
SFTTrainer loss function and formatting_func Beginners |          |   8  |  157 |  [[en.dates.tiny.about_x_hours count=19]](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</t/sfttrainer-loss-function-and-formatting-func/170499/9>)  
Recommendation of AI services Beginners |          |   4  |  58 |  [[en.dates.tiny.x_days count=1]](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</t/recommendation-of-ai-services/172218/5>)  
Minimal Transformer Modification: Memory Tokens + Gated MLP Improves Consistency Beginners |      |   1  |  19 |  [[en.dates.tiny.x_days count=1]](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</t/minimal-transformer-modification-memory-tokens-gated-mlp-improves-consistency/172526/2>)  
Finetuning T5 problems Models |      |   12  |  123 |  [[en.dates.tiny.x_days count=1]](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</t/finetuning-t5-problems/168225/13>)  
AI Recruiting Assistant - A Hugging Face Space by 19arjun89 Spaces |    |   0  |  9 |  [[en.dates.tiny.x_days count=1]](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</t/ai-recruiting-assistant-a-hugging-face-space-by-19arjun89/172533/1>)  
Proposal: A Control Plane Protocol for AI Agents (AIDP) Show and Tell |    |   0  |  7 |  [[en.dates.tiny.x_days count=1]](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</t/proposal-a-control-plane-protocol-for-ai-agents-aidp/172530/1>)  
Change Trending Spaces + MCP Spaces |    |   0  |  10 |  [[en.dates.tiny.x_days count=1]](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</t/change-trending-spaces-mcp/172529/1>)  
Distributed LLaMA Inference Engine Built from Scratch (KV Cache, GQA, RoPE) ü§óTransformers |    |   0  |  18 |  [[en.dates.tiny.x_days count=1]](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</t/distributed-llama-inference-engine-built-from-scratch-kv-cache-gqa-rope/172528/1>)  
Runtime Error message Spaces |    |   0  |  8 |  [[en.dates.tiny.x_days count=1]](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</t/runtime-error-message/172527/1>)  
Run name issue, different run name file in webpage & local ü§óTransformers |      |   1  |  82 |  [[en.dates.tiny.x_days count=1]](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</t/run-name-issue-different-run-name-file-in-webpage-local/97354/2>)  
Best model for translating English to Japanese Models |            |   9  |  [en.number.short.thousands] |  [[en.dates.tiny.x_days count=2]](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</t/best-model-for-translating-english-to-japanese/99415/10>)  
LoRA Training ËÆ≠ÁªÉ Beginners |      |   1  |  27 |  [[en.dates.tiny.x_days count=2]](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</t/lora-training/172509/2>)  
Money was cut out, but the Pro subscription is not activated Beginners |        |   5  |  42 |  [[en.dates.tiny.x_days count=2]](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</t/money-was-cut-out-but-the-pro-subscription-is-not-activated/172480/6>)  
Seeking Feedback: Professional Marble & Stone Defect Dataset (Computer Vision) ü§óDatasets |    |   0  |  17 |  [[en.dates.tiny.x_days count=2]](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</t/seeking-feedback-professional-marble-stone-defect-dataset-computer-vision/172505/1>)  
‚ÄúHow do you preserve agent state across restarts?‚Äù Models |          |   4  |  80 |  [[en.dates.tiny.x_days count=2]](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</t/how-do-you-preserve-agent-state-across-restarts/172174/5>)  
Are you monetizing your AI models and how? ü§óHub |            |   6  |  346 |  [[en.dates.tiny.x_days count=3]](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</t/are-you-monetizing-your-ai-models-and-how/168783/7>)  
Non tech individual vibe coding Beginners |        |   7  |  59 |  [[en.dates.tiny.x_days count=3]](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</t/non-tech-individual-vibe-coding/172471/9>)  
Vespa - Custom tokenization in DocumentProcessor - best practices for sending processed tokens to content nodes Research |      |   1  |  13 |  [[en.dates.tiny.x_days count=3]](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</t/vespa-custom-tokenization-in-documentprocessor-best-practices-for-sending-processed-tokens-to-content-nodes/172467/2>)  
Field-specific analyzer chains in Vespa Research |      |   1  |  15 |  [[en.dates.tiny.x_days count=3]](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</t/field-specific-analyzer-chains-in-vespa/172466/2>)  
Add Discussion on the main Hugging face site Site Feedback |    |   0  |  13 |  [[en.dates.tiny.x_days count=3]](https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/</t/add-discussion-on-the-main-hugging-face-site/172487/1>)  
Invalid date  Invalid date 
