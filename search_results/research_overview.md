# Research Sources Overview
**Generated:** 2026-01-18T01:41:33.504524Z
**Total Sources:** 50 articles
**Total Words Available:** 146,880 words across all sources

## Corpus Size Breakdown
| Category | Count | Description |
|----------|-------|-------------|
| **Batch-Safe** | 31 | Under 2,500 words - safe for batch reading |
| **Medium** | 15 | 2,500-5,000 words - included in batch |
| **Large** | 4 | Over 5,000 words - read individually |

⚠️ **Batch Limit:** `read_research_files` stops at **50,000 words** cumulative.
If you need large files, read them individually with `read_local_file`.

---

## Batch-Safe Files (Recommended for batch read)

| # | File | Words | Title |
|---|------|-------|-------|
| 1 | `crawl_420f7ab9558c.md` | 404 | Where product teams design, test and optimize agen... |
| 7 | `crawl_f2dd5e2a22c7.md` | 2,336 | Vertex AI  |  Google Cloud Documentation... |
| 9 | `crawl_78069e98af83.md` | 7 | 403: Forbidden... |
| 10 | `crawl_5e9af432bb8d.md` | 77 | llama3/eval_details.md at main · meta-llama/llama3... |
| 16 | `crawl_231755893033.md` | 684 | Gemma 3 — Megatron Bridge... |
| 17 | `crawl_af7fa7b062a4.md` | 177 | Key Insights and Best Practices on Instruction Tun... |
| 18 | `crawl_a2a370c28f97.md` | 2,116 | How to Fine-Tune LLaMA 3 for Customer Support Task... |
| 19 | `crawl_2a5205a88319.md` | 2,062 | Fine-Tuning Llama3 with Chat Data — torchtune 0.3 ... |
| 21 | `crawl_1abd3a819d98.md` | 1,542 | How to Fine-tune Llama 3.1. Step by Step Guide | F... |
| 22 | `crawl_6a4ed181c5e0.md` | 880 | Llama 3.3 | Model Cards and Prompt formats... |
| 23 | `crawl_5ef05771b33f.md` | 1,109 | Runpod | The cloud built for AI... |
| 24 | `crawl_e202b2822204.md` | 2,444 | RAM & VRAM for 70B AI Models: A Complete Hardware ... |
| 25 | `crawl_3a7181eb76a4.md` | 666 | [en.filters.with_topics] - Hugging Face Forums... |
| 26 | `crawl_bfe1c852ea6f.md` | 0 | Calculating GPU Requirements for Efficient LLAMA 3... |
| 28 | `crawl_8eb2d917e559.md` | 253 | FSDP + QLoRA – Axolotl... |
| 29 | `crawl_d3ee7cd19a47.md` | 1,110 | Can I finetune llama3.3 using axolotl? · axolotl-a... |
| 31 | `crawl_61e125bb53a3.md` | 760 | GitHub - meta-llama/llama-cookbook: Welcome to the... |
| 32 | `crawl_ce36ddf2c1bc.md` | 77 | File not found · GitHub... |
| 33 | `crawl_6dea6571ff5d.md` | 656 | Examples & Use Cases... |
| 35 | `crawl_c95469a0b35b.md` | 184 | Tutorials: How To Fine-tune & Run LLMs | Unsloth D... |
| 36 | `crawl_a9e8134ff196.md` | 288 | unsloth/llama-3-70b-Instruct-bnb-4bit · Hugging Fa... |
| 38 | `crawl_2b680c3debdf.md` | 2,060 | Fine-tuning LLMs Guide | Unsloth Documentation... |
| 40 | `crawl_14a5892fe2ca.md` | 666 | [en.filters.with_topics] - Hugging Face Forums... |
| 41 | `crawl_1c14e42c1913.md` | 2,101 | DeepSpeed... |
| 42 | `crawl_56181f76db27.md` | 1,697 | Latest News - DeepSpeed... |
| 43 | `crawl_ad5fe85b20e5.md` | 1,709 | Introducing PyTorch Fully Sharded Data Parallel (F... |
| 44 | `crawl_e0e2be5a918d.md` | 2,465 | Maximizing training throughput using PyTorch FSDP ... |
| 45 | `crawl_8a7e3fe4555b.md` | 2,370 | Train models with billions of parameters using FSD... |
| 46 | `crawl_1702aa67613b.md` | 1,697 | Latest News - DeepSpeed... |
| 47 | `crawl_29cd812a9d09.md` | 788 | Case-based Research | Llama case studies... |
| 49 | `crawl_5b7364a95767.md` | 704 | Open Source Data Labeling | Label Studio... |

## Medium Files (Included in batch if space)

| # | File | Words | Title |
|---|------|-------|-------|
| 2 | `crawl_dcc1c2444d53.md` | 2,905 | Fine-tuning | How-to guides... |
| 3 | `crawl_64ad02322689.md` | 2,961 | Introducing Meta Llama 3: The most capable openly ... |
| 4 | `crawl_d04dcd95be1f.md` | 4,960 | Fine-Tuning Llama 3 with LoRA: Step-by-Step Guide... |
| 5 | `crawl_53620ab6c83f.md` | 3,572 | Finetuning LLMs with LoRA and QLoRA: Insights from... |
| 8 | `crawl_fc174fd49e2d.md` | 3,188 | A step-by-step guide to fine-tuning LLaMA 3 using ... |
| 11 | `crawl_c28985b2ac05.md` | 2,942 | NousResearch/Meta-Llama-3-70B · Hugging Face... |
| 12 | `crawl_09f07d1a5d49.md` | 3,398 | unsloth/Llama-3.3-70B-Instruct · Hugging Face... |
| 14 | `crawl_4fe6850459f1.md` | 3,045 | LoRA fine-tuning Hyperparameters Guide | Unsloth D... |
| 15 | `crawl_02e7fcb15c2f.md` | 3,045 | LoRA fine-tuning Hyperparameters Guide | Unsloth D... |
| 20 | `crawl_b487061c5e22.md` | 2,841 | Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth... |
| 30 | `crawl_991a24df103a.md` | 4,844 | meta-llama/Meta-Llama-3-70B · Hugging Face... |
| 34 | `crawl_db0b224d266a.md` | 3,194 | Tutorial: How to Finetune Llama-3 and Use In Ollam... |
| 37 | `crawl_94e95464bdd2.md` | 3,194 | Tutorial: How to Finetune Llama-3 and Use In Ollam... |
| 39 | `crawl_4242e51e337a.md` | 3,156 | GitHub - unslothai/unsloth: Fine-tuning & Reinforc... |
| 48 | `crawl_324ec3d3f437.md` | 4,071 | The Business Case for Fine-Tuning Llama 3 Today | ... |

## ⚠️ Large Files (Read Individually)

| # | File | Words | Title | Command |
|---|------|-------|-------|--------|
| 6 | `crawl_182f106a57b3.md` | 10,973 | Analyzing LLAMA3 Performance on Classifi... | `read_local_file(path="/home/kjdragan/lrepos/universal_agent/search_results/crawl_182f106a57b3.md")` |
| 13 | `crawl_998effa15e0f.md` | 34,751 | [2407.21783] The Llama 3 Herd of Models... | `read_local_file(path="/home/kjdragan/lrepos/universal_agent/search_results/crawl_998effa15e0f.md")` |
| 27 | `crawl_35db07b561c4.md` | 9,822 | Config Reference – Axolotl... | `read_local_file(path="/home/kjdragan/lrepos/universal_agent/search_results/crawl_35db07b561c4.md")` |
| 50 | `crawl_a84a81d26c5d.md` | 5,929 | Fine-Tuning Llama-2: Tailoring Models to... | `read_local_file(path="/home/kjdragan/lrepos/universal_agent/search_results/crawl_a84a81d26c5d.md")` |

---

## All Sources Detail

### Source 1: Where product teams design, test and optimize agents at Ente
- **File:** `crawl_420f7ab9558c.md` (404 words)
- **URL:** https://www.restack.io/p/llama-3-fine-tuning-answer-cat-ai
- **Date:** unknown

### Source 2: Fine-tuning | How-to guides
- **File:** `crawl_dcc1c2444d53.md` (2,905 words)
- **URL:** https://www.llama.com/docs/how-to-guides/fine-tuning
- **Date:** unknown

### Source 3: Introducing Meta Llama 3: The most capable openly available 
- **File:** `crawl_64ad02322689.md` (2,961 words)
- **URL:** https://ai.meta.com/blog/meta-llama-3
- **Date:** 2024-04-18

### Source 4: Fine-Tuning Llama 3 with LoRA: Step-by-Step Guide
- **File:** `crawl_d04dcd95be1f.md` (4,960 words)
- **URL:** https://neptune.ai/blog/fine-tuning-llama-3-with-lora
- **Date:** unknown

### Source 5: Finetuning LLMs with LoRA and QLoRA: Insights from Hundreds 
- **File:** `crawl_53620ab6c83f.md` (3,572 words)
- **URL:** https://lightning.ai/pages/community/lora-insights
- **Date:** unknown

### Source 6: Analyzing LLAMA3 Performance on Classification Task Using Lo
- **File:** `crawl_182f106a57b3.md` (10,973 words)
- **URL:** https://www.mdpi.com/2076-3417/15/6/3087
- **Date:** unknown

### Source 7: Vertex AI  |  Google Cloud Documentation
- **File:** `crawl_f2dd5e2a22c7.md` (2,336 words)
- **URL:** https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/lora-qlora
- **Date:** unknown

### Source 8: A step-by-step guide to fine-tuning LLaMA 3 using LoRA and Q
- **File:** `crawl_fc174fd49e2d.md` (3,188 words)
- **URL:** https://rabiloo.com/blog/a-step-by-step-guide-to-fine-tuning-llama-3-using-lora-and-qlora
- **Date:** 2016-11-30

### Source 9: 403: Forbidden
- **File:** `crawl_78069e98af83.md` (7 words)
- **URL:** https://aimodels.fyi/llm-directory/llama-3-3-70b-instruct
- **Date:** unknown

### Source 10: llama3/eval_details.md at main · meta-llama/llama3 · GitHub
- **File:** `crawl_5e9af432bb8d.md` (77 words)
- **URL:** https://github.com/meta-llama/llama3/blob/main/eval_details.md
- **Date:** unknown

### Source 11: NousResearch/Meta-Llama-3-70B · Hugging Face
- **File:** `crawl_c28985b2ac05.md` (2,942 words)
- **URL:** https://huggingface.co/NousResearch/Meta-Llama-3-70B
- **Date:** 2024-04-18

### Source 12: unsloth/Llama-3.3-70B-Instruct · Hugging Face
- **File:** `crawl_09f07d1a5d49.md` (3,398 words)
- **URL:** https://huggingface.co/unsloth/Llama-3.3-70B-Instruct
- **Date:** 2024-12-06

### Source 13: [2407.21783] The Llama 3 Herd of Models
- **File:** `crawl_998effa15e0f.md` (34,751 words)
- **URL:** https://ar5iv.labs.arxiv.org/html/2407.21783
- **Date:** 2024-07-23

### Source 14: LoRA fine-tuning Hyperparameters Guide | Unsloth Documentati
- **File:** `crawl_4fe6850459f1.md` (3,045 words)
- **URL:** https://unsloth.ai/docs/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide
- **Date:** unknown

### Source 15: LoRA fine-tuning Hyperparameters Guide | Unsloth Documentati
- **File:** `crawl_02e7fcb15c2f.md` (3,045 words)
- **URL:** https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide
- **Date:** unknown

### Source 16: Gemma 3 — Megatron Bridge
- **File:** `crawl_231755893033.md` (684 words)
- **URL:** https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/llama3.html
- **Date:** unknown

### Source 17: Key Insights and Best Practices on Instruction Tuning
- **File:** `crawl_af7fa7b062a4.md` (177 words)
- **URL:** https://aiexpjourney.substack.com/p/key-insights-and-best-practices-on
- **Date:** unknown

### Source 18: How to Fine-Tune LLaMA 3 for Customer Support Tasks
- **File:** `crawl_a2a370c28f97.md` (2,116 words)
- **URL:** https://predibase.com/blog/tutorial-how-to-fine-tune-and-serve-llama-3-for-automated-customer-support
- **Date:** 2024-04-30

### Source 19: Fine-Tuning Llama3 with Chat Data — torchtune 0.3 documentat
- **File:** `crawl_2a5205a88319.md` (2,062 words)
- **URL:** https://docs.pytorch.org/torchtune/0.3/tutorials/chat.html
- **Date:** unknown

### Source 20: Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth
- **File:** `crawl_b487061c5e22.md` (2,841 words)
- **URL:** https://huggingface.co/blog/mlabonne/sft-llama3
- **Date:** 2024-07-29

### Source 21: How to Fine-tune Llama 3.1. Step by Step Guide | FinetuneDB
- **File:** `crawl_1abd3a819d98.md` (1,542 words)
- **URL:** https://finetunedb.com/blog/how-to-fine-tune-llama-3-1
- **Date:** unknown

### Source 22: Llama 3.3 | Model Cards and Prompt formats
- **File:** `crawl_6a4ed181c5e0.md` (880 words)
- **URL:** https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3
- **Date:** unknown

### Source 23: Runpod | The cloud built for AI
- **File:** `crawl_5ef05771b33f.md` (1,109 words)
- **URL:** https://www.runpod.io/articles/guides/how-to-fine-tune-large-language-models-on-a-budget
- **Date:** 2025-12-17

### Source 24: RAM & VRAM for 70B AI Models: A Complete Hardware Guide
- **File:** `crawl_e202b2822204.md` (2,444 words)
- **URL:** https://www.arsturn.com/blog/ram-vram-for-70b-ai-model-ultimate-guide
- **Date:** unknown

### Source 25: [en.filters.with_topics] - Hugging Face Forums
- **File:** `crawl_3a7181eb76a4.md` (666 words)
- **URL:** https://discuss.huggingface.co/t/how-much-vram-and-how-many-gpus-to-fine-tune-a-70b-parameter-model-like-llama-3-1-locally/150882
- **Date:** unknown

### Source 26: Calculating GPU Requirements for Efficient LLAMA 3.1 70B Dep
- **File:** `crawl_bfe1c852ea6f.md` (0 words)
- **URL:** https://community.ibm.com/community/user/cloud/blogs/arindam-dasgupta/2024/09/18/calculating-gpu-requirements-for-efficient-llama-3
- **Date:** 2024-09-18

### Source 27: Config Reference – Axolotl
- **File:** `crawl_35db07b561c4.md` (9,822 words)
- **URL:** https://docs.axolotl.ai/docs/config-reference.html
- **Date:** unknown

### Source 28: FSDP + QLoRA – Axolotl
- **File:** `crawl_8eb2d917e559.md` (253 words)
- **URL:** https://docs.axolotl.ai/docs/fsdp_qlora.html
- **Date:** unknown

### Source 29: Can I finetune llama3.3 using axolotl? · axolotl-ai-cloud/ax
- **File:** `crawl_d3ee7cd19a47.md` (1,110 words)
- **URL:** https://github.com/axolotl-ai-cloud/axolotl/discussions/2283
- **Date:** unknown

### Source 30: meta-llama/Meta-Llama-3-70B · Hugging Face
- **File:** `crawl_991a24df103a.md` (4,844 words)
- **URL:** https://huggingface.co/meta-llama/Meta-Llama-3-70B
- **Date:** 2024-04-18

### Source 31: GitHub - meta-llama/llama-cookbook: Welcome to the Llama Coo
- **File:** `crawl_61e125bb53a3.md` (760 words)
- **URL:** https://github.com/meta-llama/llama-recipes
- **Date:** unknown

### Source 32: File not found · GitHub
- **File:** `crawl_ce36ddf2c1bc.md` (77 words)
- **URL:** https://github.com/meta-llama/llama-recipes/blob/main/docs/LLM_finetuning.md
- **Date:** unknown

### Source 33: Examples & Use Cases
- **File:** `crawl_6dea6571ff5d.md` (656 words)
- **URL:** https://run.house/examples/fine-tune-llama-3-with-lora
- **Date:** unknown

### Source 34: Tutorial: How to Finetune Llama-3 and Use In Ollama | Unslot
- **File:** `crawl_db0b224d266a.md` (3,194 words)
- **URL:** https://docs.unsloth.ai/models/tutorials-how-to-fine-tune-and-run-llms/tutorial-how-to-finetune-llama-3-and-use-in-ollama
- **Date:** unknown

### Source 35: Tutorials: How To Fine-tune & Run LLMs | Unsloth Documentati
- **File:** `crawl_c95469a0b35b.md` (184 words)
- **URL:** https://docs.unsloth.ai/models/tutorials-how-to-fine-tune-and-run-llms
- **Date:** unknown

### Source 36: unsloth/llama-3-70b-Instruct-bnb-4bit · Hugging Face
- **File:** `crawl_a9e8134ff196.md` (288 words)
- **URL:** https://huggingface.co/unsloth/llama-3-70b-Instruct-bnb-4bit
- **Date:** unknown

### Source 37: Tutorial: How to Finetune Llama-3 and Use In Ollama | Unslot
- **File:** `crawl_94e95464bdd2.md` (3,194 words)
- **URL:** https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/tutorial-how-to-finetune-llama-3-and-use-in-ollama
- **Date:** unknown

### Source 38: Fine-tuning LLMs Guide | Unsloth Documentation
- **File:** `crawl_2b680c3debdf.md` (2,060 words)
- **URL:** https://docs.unsloth.ai/get-started/fine-tuning-llms-guide
- **Date:** unknown

### Source 39: GitHub - unslothai/unsloth: Fine-tuning & Reinforcement Lear
- **File:** `crawl_4242e51e337a.md` (3,156 words)
- **URL:** https://github.com/unslothai/unsloth
- **Date:** unknown

### Source 40: [en.filters.with_topics] - Hugging Face Forums
- **File:** `crawl_14a5892fe2ca.md` (666 words)
- **URL:** https://discuss.huggingface.co/t/repetition-issues-in-llama-models-3-8b-3-70b-3-1-3-2/144196
- **Date:** unknown

### Source 41: DeepSpeed
- **File:** `crawl_1c14e42c1913.md` (2,101 words)
- **URL:** https://huggingface.co/docs/peft/v0.8.2/en/accelerate/deepspeed
- **Date:** unknown

### Source 42: Latest News - DeepSpeed
- **File:** `crawl_56181f76db27.md` (1,697 words)
- **URL:** https://deepspeed.ai/training
- **Date:** unknown

### Source 43: Introducing PyTorch Fully Sharded Data Parallel (FSDP) API –
- **File:** `crawl_ad5fe85b20e5.md` (1,709 words)
- **URL:** https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api
- **Date:** 2022-03-14

### Source 44: Maximizing training throughput using PyTorch FSDP – PyTorch
- **File:** `crawl_e0e2be5a918d.md` (2,465 words)
- **URL:** https://pytorch.org/blog/maximizing-training
- **Date:** 2024-03-13

### Source 45: Train models with billions of parameters using FSDP — PyTorc
- **File:** `crawl_8a7e3fe4555b.md` (2,370 words)
- **URL:** https://lightning.ai/docs/pytorch/stable/advanced/model_parallel/fsdp.html
- **Date:** unknown

### Source 46: Latest News - DeepSpeed
- **File:** `crawl_1702aa67613b.md` (1,697 words)
- **URL:** https://deepspeed.ai/tutorials/large-models-w-deepspeed
- **Date:** unknown

### Source 47: Case-based Research | Llama case studies
- **File:** `crawl_29cd812a9d09.md` (788 words)
- **URL:** https://www.llama.com/resources/case-studies/case_based_research
- **Date:** unknown

### Source 48: The Business Case for Fine-Tuning Llama 3 Today | Shakudo
- **File:** `crawl_324ec3d3f437.md` (4,071 words)
- **URL:** https://shakudo.io/blog/business-case-fine-tuning-llama3-today
- **Date:** 2024-05-03

### Source 49: Open Source Data Labeling | Label Studio
- **File:** `crawl_5b7364a95767.md` (704 words)
- **URL:** https://labelstud.io/blog/fine-tuning-llama-3-enhancing-accuracy-in-medical-q-and-a-with-llms
- **Date:** 2026-01-14

### Source 50: Fine-Tuning Llama-2: Tailoring Models to Unique Applications
- **File:** `crawl_a84a81d26c5d.md` (5,929 words)
- **URL:** https://anyscale.com/blog/fine-tuning-llama-2-a-comprehensive-case-study-for-tailoring-models-to-unique-applications
- **Date:** 2023-08-11

