is a sample project that you can grab off of my GitHub. And the setup is meant to be minimalistic. So, we have this start composition function that is using a red line interface here to create an interactive chat environment for the user. Within this function, I'm maintaining an array of messages. And you can see over here that I'm pushing the user's message into that array before using one of the two following functions. Generate stream is a streaming function that will gradually stream the messages response to a terminal. So the results of that within the terminal looks something like this. You can see when I give it a command, it goes ahead and streams its response right back to me. To do this, the joint function is using the typical anthropic message create method along with the stream setting. The second option that we have here is the generate response function. I'm not going to look into it, but it will be useful for our demonstration of token counting later on. This function uses the exact same message create method and simply just returns the response over here without doing any kind of streaming. In here, you can see we have a block of stream handlers and these are using the traditional stream handling methods that have been typical with previous APIs. However, these stream handlers sometimes tend to be hectic and in many cases, you don't actually want to do it like this. And thankfully, Anthropic offers a really interesting solution for this. We can simplify this greatly by making use of Anthropic's stream function here. This function doesn't need an await and also doesn't require us to build up the messages. In fact, we can actually get rid of all this completely for now. Instead, the function gives us a bunch of helper functions, and I'll put their replacements in here. Taking a look at what I've done here. After getting rid of all the previous handlers that we had, we can use a bunch of helper functions from the stream object. Stream object allows us to capture text content block and a bunch of other things here such as tools or anything else. In very many cases, it's a lot better to make use of this stream object just because of how convenient it is. With that implemented, I now have a significantly shorter function and the results of my little demo now look even better than before. With that setup working just fine, we're now ready to actually start counting tokens. So here in my normal generate response function, I can tell how many tokens the response has generated using the message which comes as a result of calling the messages function dot usage object. And this object will contain useful information about usage. And what I'm doing is stringifying it and then printing it out to the console. When it comes to a streaming function, this is a case where it becomes immediately clear why it was useful for us to use this stream function as opposed to the all generate function because of the specific format in which it gives us information and that makes it very easy for us to get access to the usage information. So you can see here we can get access to the final message a message object will always contain the usage value. So we can get that using it usage property. Once again we're stringifying it and then writing it out to the console. It's quite cool how similar these two approaches are and all that is really enabled by the use of this stream function here. Now, if I fire up my terminal and give this a run here, you can see we get the input information as well as a bunch of other token information about the message and response that the model is processing. To be fair, this doesn't look all that clean to me. So, I'll take a minute to really polish it up. To do this final optimization, we'll take advantage of the fact that both of these functions have been written to return a message object and that this message object returned over here is the one that contains the usage statistics that we need. So, right back here in our interactive terminal chat here, right where we receive this message object, we can have access to it usage properties and we can print out its input and output tokens. And as you can see, this should work with any variant of the generation function that we decide to make use of. While we're at it though, let's make sure to handle one additional specific issue with our current implementation of this token counting functionality. So, back in our generate stream function here, notice that we are only sending out a message and as such counting tokens after we've finished our generation, which is happening right around here. This is not exactly ideal because for our purpose of token counting, we're trying to optimize our generation phase. And it's quite pointless to be counting tokens when you've already finished sending in your message for generation. and as such already done an unoptimized generation. Instead, we want to be counting our token usages around here right before we send in a message for generation. Then depending on the result of our counting that we get here, we can decide whether we actually want to proceed with sending a message or if we first want to branch into an external phase, optimize our messages and then get back to actually sending in our messages. for this exact purpose. The messages object here, the same one that you use to send in messages, has a token count function. This one right over here. And this is part of the new token counting API that Anthropic provides. This function takes in the exact same message structure as what you normally send into generation functions with the exception of maximum tokens or anything else that actually controls the behavior of the model that you're using. So you can see what I'm doing here is I'm using this function to count the number of tokens and I've also made sure to format its output in a nice looking format like we want for the final measurements as well. So now if I give this a run in our terminal what you can see here is we get information about what input tokens we're sending in and then we receive our message generation and then finally we receive the metrics about what tokens were sent in as well as how many tokens have been generated by the model. During the recording of this tutorial, I almost decided to leave out the original message statistics that we used right here at the beginning. But it's worth noting that despite the introduction of the new token counting API, the original messages statistics have not become completely irrelevant. And this is for two specific reasons. The first one being the original metrics is the only way that you can actually know how many output tokens have been generated by your model. This is useful for billing scenarios. The second one is to do more with accuracy. Sometimes you'll send in an API request that will require the Anthropic API to do an additional function call on a serverside level within anthropic. This can happen for things such as using the internet search tool, which actually requires your baseline model to do an additional token generation that it then sends to the internet search model. In those specific cases, this token counting function cannot tell you how many input tokens have actually been generated for the sake of sending in a prompt that is then sent to the internet search model. Anthropic has increased this reliance on these server side functions recently. And the only way to know how many input tokens were actually generated during an intermediary step like that is to take a look at these usage statistics that come in after the final response. So all that is really to say that this token counting API here is really for the sake of approximating how many initial input tokens you're sending to anthropic and it can give you much more information like that for our specific case where we're simply using it to measure the number of tokens so that we can do an optimization. It's more than enough but it can't be used for example to estimate how much you should charge your user for a specific request. It's just not accurate enough for those kind of scenarios.
