# Stress Test Results: L5 (25+ Sources) - FAILURE

**Test ID**: stress_L5_20260105_165820  
**Date**: 2026-01-05  
**Session**: session_20260105_165820  
**Run ID**: f3fa8acb-1c10-4187-87cf-a1f99b5dc0bf  
**Trace ID**: 019b9061fcdf15463a60912623db8384

---

## Outcome: âŒ FAILURE - Context Exhaustion

---

## Query Used

```
Create an exhaustive AI industry research report. Find 25+ diverse sources covering ALL major AI labs (OpenAI, Anthropic, Google DeepMind, Meta AI, Mistral, xAI), chip manufacturers (NVIDIA, AMD), and recent regulatory developments. Produce a comprehensive 20+ page professional report with detailed company profiles, market analysis, technology trends, and specific quotes from sources. Save to work_products.
```

---

## Critical Failure Point Identified

### The Exact Failure

```
ğŸ“¦ Tool Result (1131 bytes) +124.786s
   Preview: Error: result (202,352 characters) exceeds maximum allowed tokens. 
   Output has been saved to /home/kjdragan/.claude/projects/.../tool-results/...txt.
   Format: JSON with schema: {result: string}
   Use offset and limit parameters to read specific portions...
```

### The Limit That Was Hit

| Metric | Value | Limit |
|--------|-------|-------|
| **Characters returned** | 202,352 | - |
| **Tokens (calculated)** | 50,583 | **25,000** |
| **Words in corpus** | 29,128 | 100,000 (env var) |

**Key insight**: The failure was NOT our `BATCH_MAX_TOTAL` word limit (set to 100K). It was the **Claude SDK's tool result token limit** of 25,000 tokens.

### Failure Mode: Stuck in Retry Loop

After the tool output was truncated and saved to file, the agent:
1. Tried to read with `Read` tool â†’ "File exceeds 25000 tokens"
2. Tried `Bash head -c 10000` â†’ Successfully got partial content
3. Copied file to workspace âœ…
4. Then got stuck in infinite loop calling `Write` with **empty parameters**

```
ğŸ”§ [Write] +179.828s  Input size: 0 bytes
ğŸ“¦ Tool Result: InputValidationError: The required parameter `file_path` is missing
ğŸ”§ [Write] +242.12s   Input size: 0 bytes  
ğŸ“¦ Tool Result: InputValidationError: The required parameter `file_path` is missing
ğŸ”§ [Write] +302.815s  Input size: 0 bytes
... (repeated until manual termination)
```

**This is the behavior that requires harness intervention.**

---

## Metrics

### Timing
| Metric | Value |
|--------|-------|
| Total before termination | 415+ seconds (~7 min) |
| COMPOSIO_SEARCH_TOOLS | +16.3s |
| COMPOSIO_MULTI_EXECUTE (22 searches) | +55.7s |
| Task (sub-agent delegation) | +61.4s |
| finalize_research | +97.8s |
| read_research_files (FAILURE) | +124.8s |
| First Write error | +179.8s |
| Manually terminated | ~420s |

### Research Corpus
| Metric | Value |
|--------|-------|
| Search Queries | 22 (8 AI labs + chip + regulatory) |
| Extracted URLs | 40 |
| URLs After Blacklist | 40 |
| Crawl Successful | 27 |
| Crawl Failed | 13 |
| Filtered Corpus Files | 17 |
| **Total Words** | **29,128** |

### Tool Calls Before Failure
| Phase | Count |
|-------|-------|
| Search phase | 3 |
| Sub-agent + finalize | 3 |
| Read attempts | 2 |
| Bash workarounds | 3 |
| Failed Write attempts | 5+ |
| **Total** | ~16+ |

---

## What Worked

1. âœ… COMPOSIO_SEARCH_TOOLS decomposed complex query correctly
2. âœ… COMPOSIO_MULTI_EXECUTE ran 22 parallel searches
3. âœ… finalize_research processed 40 URLs into 17 filtered files
4. âœ… Observer saved all search results
5. âœ… Sub-agent delegation worked
6. âœ… Agent attempted workaround (Bash head/cp)

## What Failed

1. âŒ read_research_files returned 50K tokens - exceeds SDK 25K limit
2. âŒ Native Read tool also exceeds limit on the fallback file
3. âŒ Agent lost ability to generate Write parameters (context pressure?)
4. âŒ No recovery mechanism - stuck in infinite retry loop
5. âŒ No report generated - test had to be manually terminated

---

## Root Cause Analysis

### Primary Issue: Tool Output Token Limit

The Claude Agent SDK has a **hard limit of 25,000 tokens** for individual tool results. When `read_research_files` returned 50,583 tokens, the SDK:
1. Saved the full output to a temp file
2. Returned an error message to the agent
3. Expected agent to use offset/limit pagination

### Secondary Issue: Context Pressure on Sub-Agent

After hitting the limit, the sub-agent appeared to lose the ability to form proper Write calls. Empty parameter errors suggest:
- Sub-agent's context may have been filled by the error handling
- Model may have been confused by the unexpected tool result format
- No graceful degradation mechanism

### Tertiary Issue: No Loop Breaker

The agent had no mechanism to:
- Detect it was in a retry loop
- Break out after N failures
- Signal for help or escalate

---

## Implications for Harness Design

### Confirmed: Need for Session Boundary Management

This test proves the exact scenario the harness must handle:
1. Agent is doing useful work
2. Context/output limit is exceeded
3. Agent becomes stuck
4. **Harness must detect stall and trigger clean restart**

### New Limit Discovered

| Limit | Value | Source |
|-------|-------|--------|
| Word limit (configurable) | 100,000 | `UA_BATCH_MAX_WORDS` env var |
| **Token output limit (SDK)** | **25,000** | Claude Agent SDK hard limit |
| Effective word limit | ~17-18K | 25K tokens â‰ˆ 18K words average |

**Recommendation**: Set `UA_BATCH_MAX_WORDS` to 18000 (more conservative) to stay under SDK limit.

### Harness Intervention Points

1. **Before overflow**: Sub-agent should checkpoint progress to file before reading large corpus
2. **On overflow**: harness should detect tool output truncation and restart sub-agent with chunk-based approach
3. **On stuck loop**: Harness should detect repeated failures and trigger clean termination + state save
4. **On restart**: Fresh agent should read progress file and continue

---

## Files Generated (Before Failure)

```
session_20260105_165820/
â”œâ”€â”€ search_results/
â”‚   â”œâ”€â”€ COMPOSIO_SEARCH_*.json (20 files)
â”‚   â”œâ”€â”€ crawl_*.md (27 files)
â”‚   â””â”€â”€ research_overview.md (14.8 KB)
â”œâ”€â”€ search_results_filtered_best/
â”‚   â””â”€â”€ crawl_*.md (17 files)
â”œâ”€â”€ research_data.txt (copied from SDK temp file)
â”œâ”€â”€ run.log
â”œâ”€â”€ trace.json
â””â”€â”€ transcript.md
```

**NO work_products generated** - sub-agent never completed report synthesis.

---

## Conclusion

**L5 (25+ sources) definitively identifies the failure threshold:**
- Safe zone: â‰¤15 filtered files, â‰¤25K words
- Danger zone: 15-20 files, 25-30K words
- Failure zone: 17+ files with 29K+ words

The current system **cannot autonomously recover** from context exhaustion. This validates the need for:
1. **Harness mode** for session management
2. **Progress files** for cross-session continuity
3. **Loop detection** for stuck agents
4. **Chunk-based reading** for large corpora
